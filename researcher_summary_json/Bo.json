{
    "name": "Bo",
    "personal background": "<Bo Yang completed his D.Phil degree (2016.10-2020.09) in the at , supervised by Profs. and . Prior to Oxford, he obtained an M.Phil degree from and a B.Eng degree from . He interned at the Augumented Reality team of (Palo Alto, CA) during his D.Phil study, and at  during his M.Phil study. As an undergraduate student, he was an exchange student at (Valencia, Spain). <Zhiyang Song has a thesis titled \"Learning general and robust representations using deep neural networks for scene-level semantic understanding\". \nHe is currently a researcher at the Department of Computer Science, University of Oxford.>\n\n<Zihang Lai has a PhD degree from the University of Hong Kong, with research experience at UCSD.> <PhD at now>",
    "research interest": "Research interests include:\n- Machine learning\n- Computer vision \n- Robotics\n- 3D world interaction and recognition, understanding, and reconstruction of all individual objects within large-scale 3D scenes. The researcher's research interest is to understand scenes and objects within them by learning general and robust representations using deep neural networks, trained on large-scale real-world 3D data. The three core contributions are object-level 3D shape estimation from single or multiple views and scene-level semantic understanding.",
    "publication": "<Title>Accepted Publications</Title>\n<Published Time>2024-05-23</Published Time>\n<Conference>TPAMI 2024</Conference>\n\nWe present the first framework to represent dynamic 3D scenes in infinitely many ways from a monocular RGB video.\n\n<Conference>ICML 2024</Conference>\n\nZ. Liu, B. Yang*, Y. Luximon, A. Kumar, J. Li\nAdvances in Neural Information Processing Systems (NeurIPS) , 2023 / \nWe propose a novel ray-based 3D shape representation, achieving a 1000x faster speed in rendering.\n\nJ. Li, Z. Song, B. Yang\nAdvances in Neural Information Processing Systems (NeurIPS) , 2023 /\nWe present a novel framework to simultaneously learn the geometry, appearance, and physical velocity of 3D scenes.\n\nZ. Zhang, B. Yang*, B. Wang, B. Li\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR) , 2023 / \nWe propose the first unsupervised 3D semantic segmentation method, learning from growing superpoints in point clouds.\n\n<Conference>ECCV 2022</Conference>\n\nZ. Song, B. Yang\nAdvances in Neural Information Processing Systems (NeurIPS) , 2022 /\nWe introduce the first unsupervised 3D object segmentation method on point clouds.\n\n<Conference>ICCV 2021</Conference>\n\nOur paper for neural rendering is accepted by ICCV 2021.\nZ. Liu, B. Yang\nOur of SensatUrban is accepted by IJCV.\nOur of RandLA-Net is accepted by TPAMI.\nWe are going to organize at ICCV 2021.\n\n<Conference>NeurIPS 2022</Conference>\n\nOur papers and are accepted by NeurIPS 2022.\n\n<Conference>ECCV 2022</Conference>\n\nOur paper is accepted by ECCV 2022. <Title: unsupervised 3D object segmentation method on point clouds</Title>\n<published time: 2020.09</published time>\n<Conference of researcher's publication: Transfer/Confirmation/Viva, </Conference of researcher's publication>\n\nThis thesis aims to understand scenes and the objects within them by learning general and robust representations using deep neural networks, trained on large-scale real-world 3D data. In particular, the thesis makes three core contributions from object-level 3D shape estimation from single or multiple views to scene-level semantic understanding.\n\n<Invited talk about 3D Point Cloud Segmentation at </Invited talk about 3D Point Cloud Segmentation at>\n\nInvited talk about our and at Shenlan. Here are the .\n\n<Regularly reviewing for top-tier conferences/journals in machine learning, computer vision, and robotics: (The Hong Kong Polytechnic University), (The Hong Kong Polytechnic University), (The Hong Kong Polytechnic University).</Regularly reviewing for top-tier conferences/journals in machine learning, computer vision, and robotics>",
    "recruitment": "<Salary:\nNo salary information is provided in the text.\n\nLab Conditions:\nThe lab conditions are not explicitly mentioned in the text. However, it can be inferred that the lab focuses on machine learning, computer vision, and robotics, with a goal to build intelligent systems for 3D reconstruction and interaction.>\n\nResearcher's Recruitment Information:\n- All emails/CVs will be carefully read and evaluated.\n- Only matched candidates will be responded. None <Researcher's Recruitment Information: None>\n<Salary: None>\n<Lab Condition: None>"
}